{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks have produced large accuracy gains in applications such as computer vision, speech recognition and natural language processing. Rapid advancements in this area have been supported by excellent libraries for developing neural networks. These libraries allow users to express neural networks in terms of a computation graph, i.e., a sequence of mathematical operations, that maps vector inputs to outputs. Given this graph, these libraries support learning network parameters from data and furthermore can optimize computation, for example, by using GPUs. The ease of developing neural networks with these libraries is a significant factor in the proliferation of these models.\n",
    "\n",
    "Unfortunately, structured prediction problems cannot be easily expressed in neural network libraries. These problems require making a set of interrelated predictions and occur frequently in natural language processing. For example, part-of-speech tagging requires assigning a part-of-speech tag to each word in a sentence, where tags for nearby words may depend on each other. Another example is dependency parsing, which requires predicting a directed tree spanning the words of a sentence. These problems can be posed as predicting a sequence of actions, such as shifts and reduces for parsing. Neural networks can be applied to these problems using locally-normalized models, which produce a probability distribution over each action given all prior actions. However, this approach is known to be suboptimal as it suffers from the *label bias* problem, which is that the scores of future actions cannot affect the scores of previous actions. *Globally-normalized models*, such as conditional random fields, solve this problem by defining a joint distribution over all sequences of actions. These models express a larger class of distributions and provide better predictions than locally-normalized models. Unfortunately, these models cannot be easily expressed or trained using existing neural network libraries.\n",
    "\n",
    "Our goal is to design a neural network library that supports structured prediction with globally-normalized models. Our observation is that *nondeterministic computation* is a natural formalism for expressing the decision space of a structured prediction problem. For example, we can express a parser as a function that nondeterministically selects a shift or reduce action, applies it to its current state, then recurses to parse the remainder of the sentence. Our library has a nondeterministic choice operator similar to McCarthy's `amb` for this purpose. We combine nondeterministic choice with a computation graph, which, as demonstrated by neural network libraries, is a powerful way for users to express models. Importantly, *the computation graph interacts with nondeterministic computation*: the scores produced by the neural network determine the probabilities of nondeterministic choices, and the nondeterministic choices determine the network's architecture. The combination of these operations results in a powerful formalism for structured prediction where users specify the decision space using nondeterminism and the prediction models using computation graphs.\n",
    "\n",
    "We implement our library as a Scala monad to leverage Scala's powerful type system, library support, and streamlined syntax for monadic operations. A instance `Pp[X]` of the monad represents a function from neural network parameters to a probability distribution over values of type `X`. The monad supports inference operations that take neural network parameters and return (an approximation of) this distribution. Running inference also generates a neural network and performs a forward pass in this network to compute its outputs, which may influence the probability distribution. The library also supports a variety of optimization algorithms for training parameters from data. These training algorithms use the approximate inference algorithms and also run backpropagation in the generated networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 new artifacts in macro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 new artifacts in runtime\n",
      "1 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/jayantk/github/pnp/\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val path = \"/Users/jayantk/github/pnp/\"\n",
    "classpath.addPath(path + \"target/scala-2.11/pnp_2.11-0.1.jar\")\n",
    "classpath.addPath(path + \"lib/jklol.jar\")\n",
    "classpath.add(\"com.google.guava\" % \"guava\" % \"17.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `Pp` monad, we statically import its functions. Nondeterministic choices are expressed using the `choose` function, which has several forms. The simplest takes an explicit list of values and their corresponding probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:23: object allenai is not a member of package org",
      "              import org.allenai.pnp.Pp ; import org.allenai.pnp.Pp._ ; val flip = { () =>",
      "                                                     ^\u001b[0m",
      "\u001b[31mMain.scala:24: not found: value choose",
      "choose(Seq(true, false), Seq(0.75, 0.25)) ",
      "^\u001b[0m",
      "\u001b[31mMain.scala:23: object allenai is not a member of package org",
      "              import org.allenai.pnp.Pp ; import org.allenai.pnp.Pp._ ; val flip = { () =>",
      "                         ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "import org.allenai.pnp.Pp\n",
    "import org.allenai.pnp.Pp._\n",
    "\n",
    "val flip = choose(Seq(true, false), Seq(0.75, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code represents a weighted coin flip that comes up `true` with probability 0.75 and `false` with probability 0.25. `flip` has type `Pp[Boolean]`, representing a distribution over values of type `Boolean`. The `Pp` object represents this distribution *lazily*, so we need to perform inference on this object to get the probability of each value. Many different inference algorithms could be used for this purpose, such as sampling. Here we perform inference using beam search to estimate the k=10 most probable outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdist\u001b[0m: \u001b[32mSeq\u001b[0m[(\u001b[32mBoolean\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArrayBuffer\u001b[0m(\u001b[33m\u001b[0m(\u001b[32mtrue\u001b[0m, \u001b[32m0.75\u001b[0m), \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.25\u001b[0m))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dist = flip.beamSearch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back the original distribution, which, while expected, is not that interesting. To construct interesting models, we need to combine multiple nondeterministic choices, which we can do conveniently using Scala's for/yield construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mthreeFlips\u001b[0m: \u001b[32mPp\u001b[0m[\u001b[32mBoolean\u001b[0m] = BindPp(CategoricalPp(List((true,-0.2876820724517809), (false,-1.3862943611198906))),<function1>)\n",
       "\u001b[36mthreeFlipsDist\u001b[0m: \u001b[32mSeq\u001b[0m[(\u001b[32mBoolean\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArrayBuffer\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32mtrue\u001b[0m, \u001b[32m0.42187500000000006\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.140625\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.140625\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.140625\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.046875000000000014\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.04687499999999999\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.04687499999999999\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32m0.015625000000000007\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val threeFlips = for {\n",
    "    flip1 <- choose(Seq(true, false), Seq(0.75, 0.25))\n",
    "    flip2 <- choose(Seq(true, false), Seq(0.75, 0.25))\n",
    "    flip3 <- choose(Seq(true, false), Seq(0.75, 0.25))\n",
    "} yield {\n",
    "    flip1 && flip2 && flip3\n",
    "}\n",
    "\n",
    "val threeFlipsDist = threeFlips.beamSearch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code flips three weighted coins and returns true if they all come up true. Inference again returns the expected distribution, which we can verify by computing 0.75 \\* 0.75 \\* 0.75 = 0.421. There are 8 entries in the returned distribution because each entry represents a way the program can execute, that is, a particular sequence of choices. Executions have additional state (specifically a computation graph) besides the return value, which is why the \"duplicate\" entries are not collapsed by inference. We'll see how to use the computation graph later on.\n",
    "\n",
    "The for/yield construction above essentially means \"make these choices sequentially and lazily\". This construction is mapped by Scala into calls to `flatMap` on the `Pp` monad, which construct a lazy representation of the probability distribution.\n",
    "\n",
    "Basically, the for/yield translates into a \n",
    "\n",
    "We can also define functions with nondeterministic choices. The `flipMany` function flips `num` coins and returns a list of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mflipMany\u001b[0m\n",
       "\u001b[36mflips\u001b[0m: \u001b[32mPp\u001b[0m[\u001b[32mList\u001b[0m[\u001b[32mBoolean\u001b[0m]] = BindPp(CategoricalPp(List((true,-0.2876820724517809), (false,-1.3862943611198906))),<function1>)\n",
       "\u001b[36mflipsDist\u001b[0m: \u001b[32mSeq\u001b[0m[(\u001b[32mList\u001b[0m[\u001b[32mBoolean\u001b[0m], \u001b[32mDouble\u001b[0m)] = \u001b[33mArrayBuffer\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[33mList\u001b[0m(\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "      \u001b[32mtrue\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flipMany(num: Int): Pp[List[Boolean]] = {\n",
    "    if (num == 0) {\n",
    "        value(List())\n",
    "    } else {\n",
    "        for {\n",
    "            flip <- choose(Seq(true, false), Seq(0.75, 0.25))\n",
    "            rest <- flipMany(num - 1)\n",
    "        } yield {\n",
    "            flip :: rest\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "val flips = flipMany(1000)\n",
    "val flipsDist = flips.beamSearch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive functions such as `flipMany` are a natural way to express many structured prediction models. We will see later how to define a sequence tagger using a function very similar to flipMany.\n",
    "\n",
    "At this point, it is worthwhile noting that inference does **not** explicitly enumerate all of the possible executions of the program, even though the for/yield syntax suggests this interpretation. We can verify that explicit enumeration does not occur by changing the argument 4 above to something large, such as 10000, that would make enumeration computationally infeasible. Inference still completes quickly because `beamSearch` only approximately searches the space of executions.\n",
    "\n",
    "The examples above demonstrate how to use `Pp` to express a distribution over values in terms of probabilistic choices and perform inference over the results. The monad also allows us to define computation graphs for neural networks. Here's a function implementing a multilayer perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.JavaConverters._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.allenai.pnp.{ Env, CompGraph, CompGraphNode }\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.jayantkrish.jklol.tensor.{ Tensor, DenseTensor }\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.jayantkrish.jklol.util.IndexedList\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.google.common.base.Preconditions\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mmultilayerPerceptron\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "import org.allenai.pnp.{ Env, CompGraph, CompGraphNode }\n",
    "import com.jayantkrish.jklol.tensor.{ Tensor, DenseTensor }\n",
    "import com.jayantkrish.jklol.util.IndexedList\n",
    "import com.google.common.base.Preconditions\n",
    "\n",
    "def multilayerPerceptron(featureVector: Tensor): Pp[CompGraphNode] = for {\n",
    "    params <- param(\"params\")\n",
    "    bias <- param(\"bias\")\n",
    "    hidden = ((params * featureVector) + bias).tanh\n",
    "    params2 <- param(\"params2\")\n",
    "    bias2 <- param(\"bias2\")\n",
    "    dist = (params2 * hidden) + bias2\n",
    "} yield {\n",
    "    dist\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function applies a two-layer neural network to an input feature vector. The feature vector is represented as a ```Tensor``` from the jklol library. Neural network parameters are retrieved by name using the ```param``` function and are instances of ```CompGraphNode```. The function builds a neural network out of these nodes by applying standard operations such as matrix/vector multiplication, addition, and the hyperbolic tangent. These operations are overloaded to create new nodes in the computation graph.\n",
    "\n",
    "Something that may seem odd is that the return type for this method is `Pp[CompGraphNode]`, that is, a distribution over computation graph nodes. The reason for this type is that operations on `CompGraphNode` are **stateful** and manipulate an underlying computation graph. However, this graph is tracked by the `Pp` monad so it doesn't need to be explicitly referenced. This type also enables the computation graph to interact with nondeterministic choices, as we will see later.\n",
    "\n",
    "Let's evaluate our neural network with a feature vector and some random parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfeatures\u001b[0m: \u001b[32mDenseTensor\u001b[0m = [DenseTensor [0]=1.0, [1]=1.0, ]\n",
       "\u001b[36mnnPp\u001b[0m: \u001b[32mPp\u001b[0m[\u001b[32mCompGraphNode\u001b[0m] = BindPp(ParameterPp(params,-1),<function1>)\n",
       "\u001b[36mcompGraph\u001b[0m: \u001b[32mCompGraph\u001b[0m = org.allenai.pnp.CompGraph@494e13f5\n",
       "\u001b[36mdist\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mExecution\u001b[0m[\u001b[32mCompGraphNode\u001b[0m]] = \u001b[33mArray\u001b[0m([Execution org.allenai.pnp.PlusNode@168c8d2d 0.0])\n",
       "\u001b[36mnnOutput\u001b[0m: \u001b[32mTensor\u001b[0m = [DenseTensor [0]=1.3619915424830156, [1]=3.2401367678101494, ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val features = new DenseTensor(\n",
    "    // Dimension specification for the tensor\n",
    "    Array[Int](2), Array[Int](2),\n",
    "    // The tensor's values\n",
    "    Array(1, 1))\n",
    "\n",
    "// Evaluate the network with the feature\n",
    "val nnPp = multilayerPerceptron(features)\n",
    "\n",
    "// Randomly initialize the network parameters\n",
    "val compGraph = new CompGraph(\n",
    "    IndexedList.create(List(\"params\", \"bias\", \"params2\", \"bias2\").asJava),\n",
    "    Array(DenseTensor.random(Array(2, 1), Array(2, 8), 0.0, 1.0),\n",
    "          DenseTensor.random(Array(1), Array(8), 0.0, 1.0),\n",
    "          DenseTensor.random(Array(0, 1), Array(2, 8), 0.0, 1.0),\n",
    "          DenseTensor.random(Array(0), Array(2), 0.0, 1.0)))\n",
    "\n",
    "val dist = nnPp.beamSearch(10, Env.init, compGraph).executions\n",
    "val nnOutput = dist(0).value.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the network we define values for the feature vector as well as the named network parameters. These parameters are declared by creating a computation graph containing them. This graph is then passed to the `beamSearch` method which performs the forward pass of inference in the neural network. Inference returns a distribution with a single `CompGraphNode` that contains the network's output as its `value`. The output is a tensor with two values, which is all we can expect given the random inputs.\n",
    "\n",
    "Again, it may seem odd that `beamSearch` computes the network's forward pass in addition to performing probabilistic inference. This overloading is intentional, as it allows us to combine neural networks with nondeterministic choices to create probability distributions. Let's use our multilayer perceptron to define a probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true 0.8673979379202605\n",
      "false 0.1326020620797394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mbooleanFunction\u001b[0m\n",
       "\u001b[36moutput\u001b[0m: \u001b[32mPp\u001b[0m[\u001b[32mBoolean\u001b[0m] = BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>)\n",
       "\u001b[36mdist\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpBeamMarginals\u001b[0m[\u001b[32mBoolean\u001b[0m] = org.allenai.pnp.PpBeamMarginals@3b9aecd"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def booleanFunction(left: Boolean, right: Boolean): Pp[Boolean] = {\n",
    "    // Build a feature vector from the inputs\n",
    "    val values = Array.ofDim[Double](2)\n",
    "    values(0) = if (left) { 1 } else { 0 }\n",
    "    values(1) = if (right) { 1 } else { 0 }\n",
    "    val featureVector = new DenseTensor(\n",
    "        Array[Int](2), Array[Int](2), values)\n",
    "  \n",
    "    for {\n",
    "        dist <- multilayerPerceptron(featureVector)\n",
    "        output <- choose(Array(false, true), dist)\n",
    "    } yield {\n",
    "        output\n",
    "    }\n",
    "}\n",
    "\n",
    "val output = booleanFunction(true, true)\n",
    "val dist = output.beamSearch(10, Env.init, compGraph)\n",
    "for (d <- dist.executions) {\n",
    "    println(d.value + \" \" + (d.prob / dist.partitionFunction))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes a distribution over boolean outputs given two boolean inputs. It encodes the inputs as a feature vector, passes this vector to the multilayer perceptron, then uses the output of the perceptron to choose the output. Note that the input '(true, true)' is encoded as the same feature vector used in the above code block, and that the network has the same parameters. If you look closely at the output of the two above cells, you'll notice that 113.15 = e^4.72 and 12.73 = e^2.54. That's because the values computed by the neural network are logspace weights for each execution. Normalizing these weights gives us a probability distribution over executions.\n",
    "\n",
    "Next, let's train the network parameters to learn the xor function. First, let's create some training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.allenai.pnp.PpExample\u001b[0m\n",
       "\u001b[36mdata\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mBoolean\u001b[0m, \u001b[32mBoolean\u001b[0m, \u001b[32mBoolean\u001b[0m)] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32mtrue\u001b[0m, \u001b[32mtrue\u001b[0m, \u001b[32mfalse\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mtrue\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32mtrue\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32mtrue\u001b[0m, \u001b[32mtrue\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32mfalse\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32mfalse\u001b[0m)\n",
       ")\n",
       "\u001b[36mexamples\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpExample\u001b[0m[\u001b[32mBoolean\u001b[0m]] = \u001b[33mList\u001b[0m(\n",
       "  PpExample(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),BindPp(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),<function1>),org.allenai.pnp.Env@6fb8673a,<function1>),\n",
       "  PpExample(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),BindPp(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),<function1>),org.allenai.pnp.Env@51d85bfd,<function1>),\n",
       "  PpExample(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),BindPp(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),<function1>),org.allenai.pnp.Env@3c871636,<function1>),\n",
       "  PpExample(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),BindPp(BindPp(BindPp(ParameterPp(params,-1),<function1>),<function1>),<function1>),org.allenai.pnp.Env@41b1423b,<function1>)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.allenai.pnp.PpExample\n",
    "\n",
    "// Create training data.\n",
    "val data = List(\n",
    "  (true, true, false),\n",
    "  (true, false, true),\n",
    "  (false, true, true),\n",
    "  (false, false, false)\n",
    ")\n",
    "val examples = data.map(x => {\n",
    "  val unconditional = booleanFunction(x._1, x._2)\n",
    "  val conditional = for {\n",
    "    y <- unconditional;\n",
    "    x <- Pp.require(y == x._3)\n",
    "  } yield {\n",
    "    y\n",
    "  }\n",
    "  PpExample.fromDistributions(unconditional, conditional)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training example consists of a tuple of an unconditional and a conditional probability distribution. The unconditional distribution is generated by calling `booleanFunction`, and the conditional distribution is generated by constraining the unconditional distribution to have the correct label using ``require``. Using these examples, we can train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mcom.jayantkrish.jklol.models.DiscreteVariable\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.jayantkrish.jklol.models.VariableNumMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.jayantkrish.jklol.training.{ StochasticGradientTrainer, Lbfgs }\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.jayantkrish.jklol.training.NullLogFunction\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.allenai.pnp.ParametricPpModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.allenai.pnp.PpLoglikelihoodOracle\u001b[0m\n",
       "\u001b[36mv\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mDiscreteVariable\u001b[0m = boolean (2 values)\n",
       "\u001b[36mh\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mDiscreteVariable\u001b[0m = hidden (8 values)\n",
       "\u001b[36minputVar\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mVariableNumMap\u001b[0m = [2:input=boolean (2 values),]\n",
       "\u001b[36mhiddenVar\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mVariableNumMap\u001b[0m = [1:hidden=hidden (8 values),]\n",
       "\u001b[36moutputVar\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mVariableNumMap\u001b[0m = [0:output=boolean (2 values),]\n",
       "\u001b[36mparamNames\u001b[0m: \u001b[32mIndexedList\u001b[0m[\u001b[32mString\u001b[0m] = [params, bias, params2, bias2]\n",
       "\u001b[36mfamily\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mParametricPpModel\u001b[0m = org.allenai.pnp.ParametricPpModel@46633563\n",
       "\u001b[36moracle\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpLoglikelihoodOracle\u001b[0m[\u001b[32mBoolean\u001b[0m] = org.allenai.pnp.PpLoglikelihoodOracle@31b5d6ea\n",
       "\u001b[36mtrainer\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mtraining\u001b[0m.\u001b[32mStochasticGradientTrainer\u001b[0m = com.jayantkrish.jklol.training.StochasticGradientTrainer@41091457\n",
       "\u001b[36mparams\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mparametric\u001b[0m.\u001b[32mSufficientStatistics\u001b[0m = [TableFactor([1:hidden=hidden (8 values),2:input=boolean (2 values),])(16 weights), TableFactor([1:hidden=hidden (8 values),])(8 weights), TableFactor([0:output=boolean (2 values),1:hidden=hidden (8 values),])(16 weights), TableFactor([0:output=boolean (2 values),])(2 weights)]\n",
       "\u001b[36mtrainedParams\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mparametric\u001b[0m.\u001b[32mSufficientStatistics\u001b[0m = [TableFactor([1:hidden=hidden (8 values),2:input=boolean (2 values),])(16 weights), TableFactor([1:hidden=hidden (8 values),])(8 weights), TableFactor([0:output=boolean (2 values),1:hidden=hidden (8 values),])(16 weights), TableFactor([0:output=boolean (2 values),])(2 weights)]\n",
       "\u001b[36mmodel\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpModel\u001b[0m = org.allenai.pnp.PpModel@22bc6f1b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import com.jayantkrish.jklol.models.DiscreteVariable\n",
    "import com.jayantkrish.jklol.models.VariableNumMap\n",
    "import com.jayantkrish.jklol.training.{ StochasticGradientTrainer, Lbfgs }\n",
    "import com.jayantkrish.jklol.training.NullLogFunction\n",
    "import org.allenai.pnp.ParametricPpModel\n",
    "import org.allenai.pnp.PpLoglikelihoodOracle\n",
    "\n",
    "// Initialize neural net parameters and their dimensionalities.\n",
    "// TODO(jayantk): This is more verbose than necessary\n",
    "val v = DiscreteVariable.sequence(\"boolean\", 2);\n",
    "val h = DiscreteVariable.sequence(\"hidden\", 8);\n",
    "val inputVar = VariableNumMap.singleton(2, \"input\", v)\n",
    "val hiddenVar = VariableNumMap.singleton(1, \"hidden\", h)\n",
    "val outputVar = VariableNumMap.singleton(0, \"output\", v)\n",
    "val paramNames = IndexedList.create(\n",
    "   List(\"params\", \"bias\", \"params2\", \"bias2\").asJava\n",
    ")\n",
    "\n",
    "val family = new ParametricPpModel(\n",
    "  paramNames,\n",
    "  List(inputVar.union(hiddenVar), hiddenVar,\n",
    "    hiddenVar.union(outputVar), outputVar)\n",
    ");\n",
    "\n",
    "// Train model\n",
    "val oracle = new PpLoglikelihoodOracle[Boolean](100, family)\n",
    "val trainer = StochasticGradientTrainer.createWithL2Regularization(\n",
    "  1000, 1, 1.0, false, false, 10.0, 0.0, new NullLogFunction()\n",
    ")\n",
    "val params = family.getNewSufficientStatistics\n",
    "params.perturb(1.0)\n",
    "val trainedParams = trainer.train(oracle, params, examples.asJava)\n",
    "val model = family.getModelFromParameters(trainedParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO (jayantk): The syntax for declaring parameter dimensionalities is overcomplicated at the moment.\n",
    "\n",
    "The first part of the above code declares the dimensionalities for each parameter of the network, and the second part trains these parameters using 1000 iterations of stochastic gradient descent. Let's evaluate our function with the trained parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmarginals\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpBeamMarginals\u001b[0m[\u001b[32mBoolean\u001b[0m] = org.allenai.pnp.PpBeamMarginals@3caff273\n",
       "\u001b[36mdist\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mExecution\u001b[0m[\u001b[32mBoolean\u001b[0m]] = \u001b[33mArray\u001b[0m([Execution true 6.581252026716094], [Execution false -1.7194011072095412])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val marginals = booleanFunction(false, true).beamSearch(\n",
    "    100, Env.init, model.getInitialComputationGraph)\n",
    "\n",
    "val dist = marginals.executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input `(true, true)`, the predicted output is overwhelmingly likely to be `false`. You can change the arguments above to convince yourself that we have indeed learned the xor function.\n",
    "\n",
    "Of course, any neural network library can be used to learn xor. The power of the interaction between neural networks and nondeterministic choices only appears when we start doing structured prediction. Let's build a shift-reduce parser for a context free grammar. First, let's create some data structures to represent parse trees:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mParse\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mTerminal\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mNonterminal\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abstract class Parse(val pos: String) {\n",
    "  def getTokens: List[String]\n",
    "}\n",
    "\n",
    "case class Terminal(word: String, override val pos: String) extends Parse(pos) {\n",
    "  override def getTokens: List[String] = {\n",
    "    List(word)\n",
    "  }\n",
    "\n",
    "  override def toString: String = {\n",
    "    pos + \" -> \" + word\n",
    "  }\n",
    "}\n",
    "\n",
    "case class Nonterminal(left: Parse, right: Parse, override val pos: String) extends Parse(pos) {\n",
    "  override def getTokens: List[String] = {\n",
    "    left.getTokens ++ right.getTokens\n",
    "  }\n",
    "\n",
    "  override def toString: String = {\n",
    "    pos + \" -> (\" + left.toString + \", \" + right.toString + \")\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create some data structures to represent the parser's state and actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.mutable.ListBuffer\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.MultiMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.{ HashMap, Set }\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mShiftReduceState\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mAction\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mShift\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mReduce\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "import scala.collection.mutable.MultiMap\n",
    "import scala.collection.mutable.{ HashMap, Set }\n",
    "\n",
    "// The state of a shift-reduce parser consists of a buffer\n",
    "// of tokens that haven't been consumed yet and a stack\n",
    "// of parse trees spanning the consumed tokens.  \n",
    "case class ShiftReduceState(tokens: List[String], stack: List[Parse])\n",
    "\n",
    "// Classes for representing actions of a shift/reduce\n",
    "// parser.\n",
    "abstract class Action {\n",
    "  def apply(state: ShiftReduceState): ShiftReduceState\n",
    "  \n",
    "  // An action is responsible for generating the computation graph\n",
    "  // that scores it.\n",
    "  def score(state: ShiftReduceState): Pp[CompGraphNode]\n",
    "  def addParams(names: IndexedList[String], vars: ListBuffer[VariableNumMap]): Unit\n",
    "}\n",
    "\n",
    "// The shift action consumes the first token on the buffer\n",
    "// and pushes a parse tree on to the stack.\n",
    "class Shift(val word: String, val pos: String) extends Action {\n",
    "  val terminal = Terminal(word, pos)\n",
    "  val paramName = \"shift_\" + word + \"_\" + pos\n",
    "\n",
    "  override def apply(state: ShiftReduceState): ShiftReduceState = {\n",
    "    ShiftReduceState(state.tokens.tail, (terminal :: state.stack))\n",
    "  }\n",
    "\n",
    "  override def score(state: ShiftReduceState): Pp[CompGraphNode] = {\n",
    "    Pp.param(paramName)\n",
    "  }\n",
    "\n",
    "  override def addParams(names: IndexedList[String], vars: ListBuffer[VariableNumMap]): Unit = {\n",
    "    names.add(paramName)\n",
    "    vars += VariableNumMap.EMPTY\n",
    "  }\n",
    "}\n",
    "\n",
    "// The reduce action combines the top two parses on the stack\n",
    "// into a single parse.\n",
    "class Reduce(val leftPos: String, val rightPos: String, val rootPos: String) extends Action {\n",
    "  val paramName = \"reduce_\" + leftPos + \"_\" + rightPos + \"_\" + rootPos\n",
    "  \n",
    "  override def apply(state: ShiftReduceState): ShiftReduceState = {\n",
    "    val left = state.stack(1)\n",
    "    val right = state.stack(0)\n",
    "    val nonterminal = Nonterminal(left, right, rootPos)\n",
    "    ShiftReduceState(state.tokens, (nonterminal :: state.stack.drop(2)))\n",
    "  }\n",
    "\n",
    "  override def score(state: ShiftReduceState): Pp[CompGraphNode] = {\n",
    "    Pp.param(paramName)\n",
    "  }\n",
    "\n",
    "  override def addParams(names: IndexedList[String], vars: ListBuffer[VariableNumMap]): Unit = {\n",
    "    names.add(paramName)\n",
    "    vars += VariableNumMap.EMPTY\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these data structures, we can define the parser as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mPpParser\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mPpParser\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PpParser(\n",
    "    lexActions: MultiMap[String, Action],\n",
    "    grammarActions: MultiMap[(String, String), Action]\n",
    ") {\n",
    "\n",
    "  def parse(sent: List[String]): Pp[Parse] = {\n",
    "    parse(ShiftReduceState(sent, List()))\n",
    "  }\n",
    "\n",
    "  def parse(state: ShiftReduceState): Pp[Parse] = {\n",
    "    val tokens = state.tokens\n",
    "    val stack = state.stack\n",
    "    if (tokens.size == 0 && stack.size == 1) {\n",
    "      // All tokens consumed and all possible\n",
    "      // reduces performed.\n",
    "      value(stack.head)\n",
    "    } else {\n",
    "      // Queue for each possible action\n",
    "      val actions = ListBuffer[Action]()\n",
    "\n",
    "      // Queue shift actions\n",
    "      if (tokens.size > 0) {\n",
    "        val shifts = lexActions.getOrElse(tokens.head, Set())\n",
    "        actions ++= shifts\n",
    "      }\n",
    "\n",
    "      // Queue reduce actions\n",
    "      if (stack.size >= 2) {\n",
    "        val left = stack(1)\n",
    "        val right = stack(0)\n",
    "        val reduces = grammarActions.getOrElse((left.pos, right.pos), Set())\n",
    "        actions ++= reduces\n",
    "      }\n",
    "\n",
    "      for {\n",
    "        // Score each possible action, nondeterministically\n",
    "        // select one to apply, then recurse on the next\n",
    "        // parser state.\n",
    "        scores <- scoreActions(state, actions);\n",
    "        action <- choose(actions.toArray, scores)\n",
    "        p <- parse(action.apply(state))\n",
    "      } yield {\n",
    "        p\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def scoreActions(state: ShiftReduceState, actions: ListBuffer[Action]): Pp[Array[CompGraphNode]] = {\n",
    "    val scoreList = actions.foldRight(Pp.value(List[CompGraphNode]()))((action, list) =>\n",
    "      for {\n",
    "        x <- action.score(state);\n",
    "        l <- list\n",
    "      } yield {\n",
    "        x :: l\n",
    "      })\n",
    "\n",
    "    scoreList.flatMap { x => Pp.value(x.toArray) }\n",
    "  }\n",
    "\n",
    "  // Generate the parameters used in the neural network\n",
    "  // of this parser.\n",
    "  def getParams: ParametricPpModel = {\n",
    "    val paramNames = IndexedList.create[String]()\n",
    "    val paramVars = ListBuffer[VariableNumMap]()\n",
    "\n",
    "    lexActions.values.foreach(_.foreach(_.addParams(paramNames, paramVars)))\n",
    "    grammarActions.values.foreach(_.foreach(_.addParams(paramNames, paramVars)))\n",
    "\n",
    "    new ParametricPpModel(paramNames, paramVars.toList)\n",
    "  }\n",
    "}\n",
    "\n",
    "object PpParser {\n",
    "  // Initialize parser actions from maps of string -> part of speech\n",
    "  // for shift actions and (pos, pos) -> pos for reduce actions\n",
    "  def fromMaps(\n",
    "    lexicon: List[(String, String)],\n",
    "    grammar: List[((String, String), String)]\n",
    "  ): PpParser = {\n",
    "\n",
    "    val lexActions = new HashMap[String, Set[Action]] with MultiMap[String, Action]\n",
    "    for ((k, v) <- lexicon) {\n",
    "      lexActions.addBinding(k, new Shift(k, v))\n",
    "    }\n",
    "\n",
    "    val grammarActions = new HashMap[(String, String), Set[Action]] with MultiMap[(String, String), Action]\n",
    "    for ((k, v) <- grammar) {\n",
    "      grammarActions.addBinding(k, new Reduce(k._1, k._2, v))\n",
    "    }\n",
    "\n",
    "    new PpParser(lexActions, grammarActions)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All said and done, we've built a globally-normalized shift-reduce parser with neural network factors in less than 200 lines of code, much of which is simple data structures. The scoring function for actions is overly simple, but this is easy to improve using the computation graph operations as we saw in the xor example. Let's use the parser to parse some sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlexicon\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mString\u001b[0m)] = \u001b[33mList\u001b[0m(\u001b[33m\u001b[0m(\u001b[32m\"the\"\u001b[0m, \u001b[32m\"DT\"\u001b[0m), \u001b[33m\u001b[0m(\u001b[32m\"the\"\u001b[0m, \u001b[32m\"NN\"\u001b[0m), \u001b[33m\u001b[0m(\u001b[32m\"blue\"\u001b[0m, \u001b[32m\"NN\"\u001b[0m), \u001b[33m\u001b[0m(\u001b[32m\"man\"\u001b[0m, \u001b[32m\"NN\"\u001b[0m))\n",
       "\u001b[36mgrammar\u001b[0m: \u001b[32mList\u001b[0m[((\u001b[32mString\u001b[0m, \u001b[32mString\u001b[0m), \u001b[32mString\u001b[0m)] = \u001b[33mList\u001b[0m(\u001b[33m\u001b[0m(\u001b[33m\u001b[0m(\u001b[32m\"DT\"\u001b[0m, \u001b[32m\"NN\"\u001b[0m), \u001b[32m\"NP\"\u001b[0m), \u001b[33m\u001b[0m(\u001b[33m\u001b[0m(\u001b[32m\"NN\"\u001b[0m, \u001b[32m\"NN\"\u001b[0m), \u001b[32m\"NN\"\u001b[0m))\n",
       "\u001b[36mparser\u001b[0m: \u001b[32mPpParser\u001b[0m = cmd13$$user$PpParser@73815acf\n",
       "\u001b[36mfamily\u001b[0m: \u001b[32mParametricPpModel\u001b[0m = org.allenai.pnp.ParametricPpModel@39b76121\n",
       "\u001b[36mmodel\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpModel\u001b[0m = org.allenai.pnp.PpModel@2dfe7c9c\n",
       "\u001b[36mcg\u001b[0m: \u001b[32mCompGraph\u001b[0m = org.allenai.pnp.CompGraph@df178\n",
       "\u001b[36mdist\u001b[0m: \u001b[32mPp\u001b[0m[\u001b[32mParse\u001b[0m] = BindPp(BindPp(BindPp(ParameterPp(shift_the_DT,-1),<function1>),<function1>),<function1>)\n",
       "\u001b[36mmarginals\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpBeamMarginals\u001b[0m[\u001b[32mParse\u001b[0m] = org.allenai.pnp.PpBeamMarginals@5aadd72a\n",
       "\u001b[36mparses\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mExecution\u001b[0m[\u001b[32mParse\u001b[0m]] = \u001b[33mArray\u001b[0m(\n",
       "  [Execution NP -> (DT -> the, NN -> (NN -> blue, NN -> man)) 0.0],\n",
       "  [Execution NN -> (NN -> the, NN -> (NN -> blue, NN -> man)) 0.0],\n",
       "  [Execution NN -> (NN -> (NN -> the, NN -> blue), NN -> man) 0.0]\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// The set of shift actions\n",
    "val lexicon = List(\n",
    "  (\"the\", \"DT\"),\n",
    "  (\"the\", \"NN\"),\n",
    "  (\"blue\", \"NN\"),\n",
    "  (\"man\", \"NN\")\n",
    ")\n",
    "\n",
    "// The set of reduce actions\n",
    "val grammar = List(\n",
    "  ((\"DT\", \"NN\"), \"NP\"),\n",
    "  ((\"NN\", \"NN\"), \"NN\")\n",
    ")\n",
    "\n",
    "val parser = PpParser.fromMaps(lexicon, grammar)\n",
    "\n",
    "// Get the neural network parameters needed to score\n",
    "// parses in dist\n",
    "val family = parser.getParams\n",
    "val model = family.getModelFromParameters(\n",
    "    family.getNewSufficientStatistics)\n",
    "val cg = model.getInitialComputationGraph\n",
    "\n",
    "// Run inference\n",
    "val dist = parser.parse(List(\"the\", \"blue\", \"man\"))\n",
    "val marginals = dist.beamSearch(100, Env.init, cg)\n",
    "val parses = marginals.executions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing seems to work. We initialized the parameters to 0, so we get a uniform distribution over parse trees for the sentence. Now let's train the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingData\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mParse\u001b[0m] = \u001b[33mList\u001b[0m(NP -> (DT -> the, NN -> man), NN -> (NN -> blue, NN -> man))\n",
       "\u001b[36mexamples\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mPpExample\u001b[0m[\u001b[32mParse\u001b[0m]] = \u001b[33mList\u001b[0m(\n",
       "  PpExample(BindPp(BindPp(BindPp(ParameterPp(shift_the_DT,-1),<function1>),<function1>),<function1>),BindPp(BindPp(BindPp(BindPp(ParameterPp(shift_the_DT,-1),<function1>),<function1>),<function1>),<function1>),org.allenai.pnp.Env@21f8a7b2,<function1>),\n",
       "  PpExample(BindPp(BindPp(BindPp(ParameterPp(shift_blue_NN,-1),<function1>),<function1>),<function1>),BindPp(BindPp(BindPp(BindPp(ParameterPp(shift_blue_NN,-1),<function1>),<function1>),<function1>),<function1>),org.allenai.pnp.Env@54f9b8b9,<function1>)\n",
       ")\n",
       "\u001b[36moracle\u001b[0m: \u001b[32mPpLoglikelihoodOracle\u001b[0m[\u001b[32mParse\u001b[0m] = org.allenai.pnp.PpLoglikelihoodOracle@120a34cf\n",
       "\u001b[36mtrainer\u001b[0m: \u001b[32mStochasticGradientTrainer\u001b[0m = com.jayantkrish.jklol.training.StochasticGradientTrainer@590acf05\n",
       "\u001b[36mparams\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mparametric\u001b[0m.\u001b[32mSufficientStatistics\u001b[0m = [TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights)]\n",
       "\u001b[36mtrainedParams\u001b[0m: \u001b[32mcom\u001b[0m.\u001b[32mjayantkrish\u001b[0m.\u001b[32mjklol\u001b[0m.\u001b[32mmodels\u001b[0m.\u001b[32mparametric\u001b[0m.\u001b[32mSufficientStatistics\u001b[0m = [TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights), TableFactor([])(1 weights)]\n",
       "\u001b[36mmodel\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpModel\u001b[0m = org.allenai.pnp.PpModel@3dab26a1\n",
       "\u001b[36mdist\u001b[0m: \u001b[32mPp\u001b[0m[\u001b[32mParse\u001b[0m] = BindPp(BindPp(BindPp(ParameterPp(shift_the_DT,-1),<function1>),<function1>),<function1>)\n",
       "\u001b[36mmarginals\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mPpBeamMarginals\u001b[0m[\u001b[32mParse\u001b[0m] = org.allenai.pnp.PpBeamMarginals@248e7ddd\n",
       "\u001b[36mparses\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mallenai\u001b[0m.\u001b[32mpnp\u001b[0m.\u001b[32mExecution\u001b[0m[\u001b[32mParse\u001b[0m]] = \u001b[33mArray\u001b[0m(\n",
       "  [Execution NP -> (DT -> the, NN -> (NN -> blue, NN -> man)) 1.9013592629203682],\n",
       "  [Execution NN -> (NN -> (NN -> the, NN -> blue), NN -> man) -5.70407778876114],\n",
       "  [Execution NN -> (NN -> the, NN -> (NN -> blue, NN -> man)) -5.70407778876114]\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val trainingData = List[Parse](\n",
    "    Nonterminal(Terminal(\"the\", \"DT\"), Terminal(\"man\", \"NN\"), \"NP\"),\n",
    "    Nonterminal(Terminal(\"blue\", \"NN\"), Terminal(\"man\", \"NN\"), \"NN\")\n",
    "    )\n",
    "\n",
    "// Convert the trees to unconditional / conditional distributions\n",
    "// that are used for training.\n",
    "val examples = trainingData.map(tree => {\n",
    "    val unconditional = parser.parse(tree.getTokens)\n",
    "    val conditional = for {\n",
    "        parse <- unconditional;\n",
    "        _ <- Pp.require(parse == tree)\n",
    "    } yield {\n",
    "        parse\n",
    "    }\n",
    "    PpExample.fromDistributions(unconditional, conditional)\n",
    "})\n",
    "\n",
    "val oracle = new PpLoglikelihoodOracle[Parse](100, family)\n",
    "val trainer = StochasticGradientTrainer.createWithL2Regularization(\n",
    "  1000, 1, 1.0, false, false, 10.0, 0.0, new NullLogFunction())\n",
    "val params = family.getNewSufficientStatistics\n",
    "val trainedParams = trainer.train(oracle, params, examples.asJava)\n",
    "val model = family.getModelFromParameters(trainedParams)\n",
    "\n",
    "val dist = parser.parse(List(\"the\", \"blue\", \"man\"))\n",
    "val marginals = dist.beamSearch(100, Env.init, model.getInitialComputationGraph)\n",
    "val parses = marginals.executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is nearly identical to the code for the xor example. We generate unconditional/conditional distributions over parse trees, then maximize loglikelihood with stochastic gradient descent. Parsing the same sentence now gives us a highly peaked distribution on the tree present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:23: not found: type Pp",
      "              def answerQ(question: String): Pp[String] = {",
      "                                             ^\u001b[0m",
      "\u001b[31mMain.scala:30: not found: type Pp",
      "} ; def chooseSentence(question: String): Pp[String] = {",
      "                                          ^\u001b[0m",
      "\u001b[31mMain.scala:31: not found: value sentences",
      "    sentences = retrieveSentences(question)",
      "    ^\u001b[0m",
      "\u001b[31mMain.scala:33: not found: value map",
      "        scores <- map(sentences, sentenceNN _)",
      "                  ^\u001b[0m",
      "\u001b[31mMain.scala:33: not found: value sentences",
      "        scores <- map(sentences, sentenceNN _)",
      "                      ^\u001b[0m",
      "\u001b[31mMain.scala:33: not found: value sentenceNN",
      "        scores <- map(sentences, sentenceNN _)",
      "                                 ^\u001b[0m",
      "\u001b[31mMain.scala:34: not found: value choose",
      "        sent <- choose(sentences, scores)",
      "                ^\u001b[0m",
      "\u001b[31mMain.scala:34: not found: value sentences",
      "        sent <- choose(sentences, scores)",
      "                       ^\u001b[0m",
      "\u001b[31mMain.scala:38: not found: type Pp",
      "} ; def sentenceNn(sentence: String): Pp[Expression] = {",
      "                                      ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "def answerQ(question: String): Pp[String] = {\n",
    "    for {\n",
    "        sentence <- chooseSentence(question)\n",
    "        answer <- chooseAnswer(sentence)\n",
    "    } yield {\n",
    "        answer\n",
    "    }\n",
    "}\n",
    "\n",
    "def chooseSentence(question: String): Pp[String] = {\n",
    "    sentences = retrieveSentences(question)\n",
    "    for {\n",
    "        scores <- map(sentences, sentenceNN _)\n",
    "        sent <- choose(sentences, scores)\n",
    "    } yield {\n",
    "        sent\n",
    "    }\n",
    "}\n",
    "\n",
    "def sentenceNn(sentence: String): Pp[Expression] = {\n",
    "    // Build a neural net to score the sentence\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:24: not found: value answerQ",
      "List((answerQ(\"The thermometer ...\"), \"temperature\"),",
      "      ^\u001b[0m",
      "\u001b[31mMain.scala:25: not found: value answerQ",
      "                (answerQ(\"What season occurs when ...\"), \"summer\")) ",
      "                 ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "val data = List((answerQ(\"The thermometer ...\"), \"temperature\"),\n",
    "                (answerQ(\"What season occurs when ...\"), \"summer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
